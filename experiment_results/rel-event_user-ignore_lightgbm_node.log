Loading Database object from /home/dengmiao/.cache/relbench/rel-event/db...
Done in 3.47 seconds.
Embedding raw data in mini-batch:   0%|          | 0/76 [00:00<?, ?it/s]Embedding raw data in mini-batch:  20%|█▉        | 15/76 [00:00<00:00, 146.20it/s]Embedding raw data in mini-batch:  54%|█████▍    | 41/76 [00:00<00:00, 209.30it/s]Embedding raw data in mini-batch:  87%|████████▋ | 66/76 [00:00<00:00, 227.33it/s]Embedding raw data in mini-batch: 100%|██████████| 76/76 [00:00<00:00, 218.27it/s]
Embedding raw data in mini-batch:   0%|          | 0/76 [00:00<?, ?it/s]Embedding raw data in mini-batch:   1%|▏         | 1/76 [00:00<00:41,  1.81it/s]Embedding raw data in mini-batch:  29%|██▉       | 22/76 [00:00<00:01, 44.27it/s]Embedding raw data in mini-batch:  57%|█████▋    | 43/76 [00:00<00:00, 81.06it/s]Embedding raw data in mini-batch:  83%|████████▎ | 63/76 [00:00<00:00, 109.69it/s]Embedding raw data in mini-batch: 100%|██████████| 76/76 [00:00<00:00, 82.44it/s] 
Embedding raw data in mini-batch:   0%|          | 0/17 [00:00<?, ?it/s]Embedding raw data in mini-batch: 100%|██████████| 17/17 [00:00<00:00, 262.92it/s]
Embedding raw data in mini-batch:   0%|          | 0/17 [00:00<?, ?it/s]Embedding raw data in mini-batch: 100%|██████████| 17/17 [00:00<00:00, 203.56it/s]
Embedding raw data in mini-batch:   0%|          | 0/16 [00:00<?, ?it/s]Embedding raw data in mini-batch: 100%|██████████| 16/16 [00:00<00:00, 265.28it/s]
Embedding raw data in mini-batch:   0%|          | 0/16 [00:00<?, ?it/s]Embedding raw data in mini-batch: 100%|██████████| 16/16 [00:00<00:00, 202.27it/s]
[I 2025-06-29 01:31:26,270] A new study created in memory with name: no-name-3a650d9e-45e6-4f72-9a0c-84a6d2bbe0e8
[I 2025-06-29 01:31:28,976] Trial 0 finished with value: 0.8036831719867434 and parameters: {'max_depth': 3, 'learning_rate': 0.0027475102947594615, 'num_leaves': 378, 'subsample': 0.18958168023890193, 'colsample_bytree': 0.7181027546248275, 'lambda_l1': 0.0003049766652807389, 'lambda_l2': 1.6763511773306988e-08, 'min_data_in_leaf': 45}. Best is trial 0 with value: 0.8036831719867434.
[I 2025-06-29 01:31:30,615] Trial 1 finished with value: 0.8277279929065644 and parameters: {'max_depth': 7, 'learning_rate': 0.008100956167204023, 'num_leaves': 645, 'subsample': 0.5991368841288924, 'colsample_bytree': 0.25933280923854196, 'lambda_l1': 7.087035150783992e-07, 'lambda_l2': 0.0008593733230600698, 'min_data_in_leaf': 63}. Best is trial 1 with value: 0.8277279929065644.
[LightGBM] [Fatal] Reducing `min_data_in_leaf` with `feature_pre_filter=true` may cause unexpected behaviour for features that were pre-filtered by the larger `min_data_in_leaf`.
You need to set `feature_pre_filter=false` to dynamically change the `min_data_in_leaf`.
[LightGBM] [Fatal] Reducing `min_data_in_leaf` with `feature_pre_filter=true` may cause unexpected behaviour for features that were pre-filtered by the larger `min_data_in_leaf`.
You need to set `feature_pre_filter=false` to dynamically change the `min_data_in_leaf`.
[I 2025-06-29 01:31:34,784] Trial 2 finished with value: 0.8147863490125395 and parameters: {'max_depth': 9, 'learning_rate': 0.0012912234222667764, 'num_leaves': 675, 'subsample': 0.28047772031376494, 'colsample_bytree': 0.1507328207705565, 'lambda_l1': 2.619279792714493e-08, 'lambda_l2': 1.0108823313976082e-06, 'min_data_in_leaf': 27}. Best is trial 1 with value: 0.8277279929065644.
[I 2025-06-29 01:31:48,231] Trial 3 finished with value: 0.8230729257514972 and parameters: {'max_depth': 6, 'learning_rate': 0.009720312243772457, 'num_leaves': 348, 'subsample': 0.09305804855491448, 'colsample_bytree': 0.4215194056403562, 'lambda_l1': 0.787742072081873, 'lambda_l2': 6.504065960395487, 'min_data_in_leaf': 58}. Best is trial 1 with value: 0.8277279929065644.
[LightGBM] [Fatal] Reducing `min_data_in_leaf` with `feature_pre_filter=true` may cause unexpected behaviour for features that were pre-filtered by the larger `min_data_in_leaf`.
You need to set `feature_pre_filter=false` to dynamically change the `min_data_in_leaf`.
[LightGBM] [Fatal] Reducing `min_data_in_leaf` with `feature_pre_filter=true` may cause unexpected behaviour for features that were pre-filtered by the larger `min_data_in_leaf`.
You need to set `feature_pre_filter=false` to dynamically change the `min_data_in_leaf`.
[I 2025-06-29 01:31:53,355] Trial 4 finished with value: 0.8247221253173636 and parameters: {'max_depth': 7, 'learning_rate': 0.0016246347726700987, 'num_leaves': 876, 'subsample': 0.6913399040533401, 'colsample_bytree': 0.8800926678364652, 'lambda_l1': 8.681990206402139, 'lambda_l2': 0.0003292919455584946, 'min_data_in_leaf': 7}. Best is trial 1 with value: 0.8277279929065644.
[I 2025-06-29 01:32:01,043] Trial 5 finished with value: 0.8506932331337094 and parameters: {'max_depth': 4, 'learning_rate': 0.09063102388071445, 'num_leaves': 1023, 'subsample': 0.46967975814861185, 'colsample_bytree': 0.32607028369856084, 'lambda_l1': 0.001676395247318512, 'lambda_l2': 2.6436578748299777e-09, 'min_data_in_leaf': 33}. Best is trial 5 with value: 0.8506932331337094.
[I 2025-06-29 01:32:15,925] Trial 6 finished with value: 0.8643604521580712 and parameters: {'max_depth': 4, 'learning_rate': 0.07394531852949421, 'num_leaves': 58, 'subsample': 0.7776936262961935, 'colsample_bytree': 0.7850032530521387, 'lambda_l1': 5.365466509818743e-07, 'lambda_l2': 4.06861787191911e-05, 'min_data_in_leaf': 36}. Best is trial 6 with value: 0.8643604521580712.
[I 2025-06-29 01:32:38,972] Trial 7 finished with value: 0.8486443010252535 and parameters: {'max_depth': 6, 'learning_rate': 0.010177428275262362, 'num_leaves': 159, 'subsample': 0.1636552422794288, 'colsample_bytree': 0.7455651594111806, 'lambda_l1': 3.233092749272927e-07, 'lambda_l2': 0.05389823866023448, 'min_data_in_leaf': 50}. Best is trial 6 with value: 0.8643604521580712.
[LightGBM] [Fatal] Reducing `min_data_in_leaf` with `feature_pre_filter=true` may cause unexpected behaviour for features that were pre-filtered by the larger `min_data_in_leaf`.
You need to set `feature_pre_filter=false` to dynamically change the `min_data_in_leaf`.
[LightGBM] [Fatal] Reducing `min_data_in_leaf` with `feature_pre_filter=true` may cause unexpected behaviour for features that were pre-filtered by the larger `min_data_in_leaf`.
You need to set `feature_pre_filter=false` to dynamically change the `min_data_in_leaf`.
[I 2025-06-29 01:33:06,075] Trial 8 finished with value: 0.8615956354051593 and parameters: {'max_depth': 8, 'learning_rate': 0.02849177150778427, 'num_leaves': 404, 'subsample': 0.15743931640765163, 'colsample_bytree': 0.6966932470964151, 'lambda_l1': 2.6523585964704225e-09, 'lambda_l2': 4.52619615169961, 'min_data_in_leaf': 4}. Best is trial 6 with value: 0.8643604521580712.
[I 2025-06-29 01:33:27,565] Trial 9 finished with value: 0.8766352694924123 and parameters: {'max_depth': 9, 'learning_rate': 0.09397316756398194, 'num_leaves': 310, 'subsample': 0.44264566230562663, 'colsample_bytree': 0.5116601605608173, 'lambda_l1': 2.0319108460576912e-09, 'lambda_l2': 5.865310777482991e-06, 'min_data_in_leaf': 23}. Best is trial 9 with value: 0.8766352694924123.
Train: {'accuracy': 0.8680804615624512, 'average_precision': 0.6414704411179286, 'f1': 0.4984189723320158, 'roc_auc': 0.8919331470478078}
Val: {'accuracy': 0.9118279569892473, 'average_precision': 0.5353414798598423, 'f1': 0.48535564853556484, 'roc_auc': 0.8766352694924123}
Test: {'accuracy': 0.8860471005317802, 'average_precision': 0.37084294921250804, 'f1': 0.36797752808988765, 'roc_auc': 0.7961798609126417}
